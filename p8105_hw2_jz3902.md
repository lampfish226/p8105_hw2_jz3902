p8105_hw2_jz3902
================
Jinghan Zhao
2024-09-27

## Problem 1

### Import and clean the data

The original dataset contains variables including: division, line,
station name, station latitude / longitude, routes served, entrance
type, entry, vending, staff and staff hours, ADA compliance and notes,
free crossover, north south / east west street, corner, entrance
latitude / longitude, station location and entrance location.

Data cleaning steps: clean variable names -\> select variables in
interest -\> convert the `entry` variable from character to a logical
variable by `case_match()`.

``` r
transit_df = 
  read_csv("HW2_data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
  mutate(
    entry = case_match(
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE
    )
  )
```

The resulting dataset has 1868 rows and 19 columns, which contains
variables including: line, station name, station latitude / longitude,
routes served, entrance type, entry, vending and ADA compliance.

### Distinct stations

Distinct stations are defined by line and station name.

``` r
transit_df %>% 
  distinct(line, station_name)
```

    ## # A tibble: 465 × 2
    ##    line     station_name            
    ##    <chr>    <chr>                   
    ##  1 4 Avenue 25th St                 
    ##  2 4 Avenue 36th St                 
    ##  3 4 Avenue 45th St                 
    ##  4 4 Avenue 53rd St                 
    ##  5 4 Avenue 59th St                 
    ##  6 4 Avenue 77th St                 
    ##  7 4 Avenue 86th St                 
    ##  8 4 Avenue 95th St                 
    ##  9 4 Avenue 9th St                  
    ## 10 4 Avenue Atlantic Av-Barclays Ctr
    ## # ℹ 455 more rows

There are 465 distinct stations.

### ADA compliant

``` r
transit_df %>%
  filter(ada == TRUE) %>% 
  distinct(line, station_name)
```

    ## # A tibble: 84 × 2
    ##    line            station_name                  
    ##    <chr>           <chr>                         
    ##  1 4 Avenue        Atlantic Av-Barclays Ctr      
    ##  2 4 Avenue        DeKalb Av                     
    ##  3 4 Avenue        Pacific St                    
    ##  4 42nd St Shuttle Grand Central                 
    ##  5 6 Avenue        34th St                       
    ##  6 6 Avenue        47-50th Sts Rockefeller Center
    ##  7 6 Avenue        Church Av                     
    ##  8 63rd Street     21st St                       
    ##  9 63rd Street     Lexington Av                  
    ## 10 63rd Street     Roosevelt Island              
    ## # ℹ 74 more rows

There are 84 ADA compliant stations.

### Vending

Calculate the numerator (allow entrance + no vending) and denominator
(no vending) respectively, then calculate the proportion.

``` r
no_vending = 
  transit_df %>%
  filter(vending == "NO") %>% 
  nrow()

entry_no_vending = 
  transit_df %>%
  filter(vending == "NO", entry == TRUE) %>% 
  nrow()

propo_vending = entry_no_vending / no_vending
```

There are 37.704918% of stations without vending allow entrance.

### Display routes

The original variable type of `route1:route7` are character, yet
`route8:route11` are numeric, so they cannot combine together.
Therefore, I convert `route8:route11` to character first, then do
`pivot_longer()`.

``` r
transit_tidy_df =
  transit_df %>% 
  mutate(
    across(route8:route11, as.character)
  ) %>% 
  pivot_longer(
    cols = route1:route11,
    names_to = "route_number",
    values_to = "route_name",
    values_drop_na = TRUE)
```

Answering questions:

``` r
transit_tidy_df %>% 
  filter(route_name == "A") %>% 
  distinct(line, station_name)
```

    ## # A tibble: 60 × 2
    ##    line            station_name                 
    ##    <chr>           <chr>                        
    ##  1 42nd St Shuttle Times Square                 
    ##  2 8 Avenue        125th St                     
    ##  3 8 Avenue        145th St                     
    ##  4 8 Avenue        14th St                      
    ##  5 8 Avenue        168th St - Washington Heights
    ##  6 8 Avenue        175th St                     
    ##  7 8 Avenue        181st St                     
    ##  8 8 Avenue        190th St                     
    ##  9 8 Avenue        34th St                      
    ## 10 8 Avenue        42nd St                      
    ## # ℹ 50 more rows

``` r
transit_tidy_df %>% 
  filter(route_name == "A", ada == TRUE) %>% 
  distinct(line, station_name)
```

    ## # A tibble: 17 × 2
    ##    line             station_name                 
    ##    <chr>            <chr>                        
    ##  1 8 Avenue         14th St                      
    ##  2 8 Avenue         168th St - Washington Heights
    ##  3 8 Avenue         175th St                     
    ##  4 8 Avenue         34th St                      
    ##  5 8 Avenue         42nd St                      
    ##  6 8 Avenue         59th St                      
    ##  7 8 Avenue         Inwood - 207th St            
    ##  8 8 Avenue         West 4th St                  
    ##  9 8 Avenue         World Trade Center           
    ## 10 Broadway         Times Square-42nd St         
    ## 11 Broadway-7th Ave 59th St-Columbus Circle      
    ## 12 Broadway-7th Ave Times Square                 
    ## 13 Canarsie         8th Av                       
    ## 14 Franklin         Franklin Av                  
    ## 15 Fulton           Euclid Av                    
    ## 16 Fulton           Franklin Av                  
    ## 17 Rockaway         Howard Beach

There are 60 distinct stations serve the A train.

There are 17 ADA compliant stations that serve the A train.

## Problem 2

### Import and clean the data

Steps: import dataset (claim the sheet and cell range) -\> clean
variable names -\> do some mutating in need

``` r
mr_trash_df = 
  read_excel("HW2_data/202409 Trash Wheel Collection Data.xlsx", 
             sheet = "Mr. Trash Wheel",
             range = "A2:N653") %>% 
  janitor::clean_names() %>% 
  mutate(
    sports_balls = round(sports_balls),
    sports_balls = as.integer(sports_balls),
    type = "mr_trash",
    year = as.numeric(year)
  )

professor_trash_df = 
  read_excel("HW2_data/202409 Trash Wheel Collection Data.xlsx", 
             sheet = "Professor Trash Wheel",
             range = "A2:M120") %>% 
  janitor::clean_names() %>% 
  mutate(
    type = "professor_trash"
  )

gwynnda_trash_df = 
  read_excel("HW2_data/202409 Trash Wheel Collection Data.xlsx", 
             sheet = "Gwynnda Trash Wheel",
             range = "A2:L265") %>% 
  janitor::clean_names() %>% 
  mutate(
    type = "gwynnda_trash"
  )
```

### Combine datasets

``` r
trash_df = 
  bind_rows(mr_trash_df, professor_trash_df, gwynnda_trash_df) %>% 
  relocate(type)
```

There are 1032 observations and 15 columns in the resulting dataset.
Here are the first few lines of the dataset to show key variables:

``` r
trash_df %>% 
  head() %>% 
  knitr::kable()
```

| type     | dumpster | month | year | date       | weight_tons | volume_cubic_yards | plastic_bottles | polystyrene | cigarette_butts | glass_bottles | plastic_bags | wrappers | sports_balls | homes_powered |
|:---------|---------:|:------|-----:|:-----------|------------:|-------------------:|----------------:|------------:|----------------:|--------------:|-------------:|---------:|-------------:|--------------:|
| mr_trash |        1 | May   | 2014 | 2014-05-16 |        4.31 |                 18 |            1450 |        1820 |          126000 |            72 |          584 |     1162 |            7 |             0 |
| mr_trash |        2 | May   | 2014 | 2014-05-16 |        2.74 |                 13 |            1120 |        1030 |           91000 |            42 |          496 |      874 |            5 |             0 |
| mr_trash |        3 | May   | 2014 | 2014-05-16 |        3.45 |                 15 |            2450 |        3100 |          105000 |            50 |         1080 |     2032 |            6 |             0 |
| mr_trash |        4 | May   | 2014 | 2014-05-17 |        3.10 |                 15 |            2380 |        2730 |          100000 |            52 |          896 |     1971 |            6 |             0 |
| mr_trash |        5 | May   | 2014 | 2014-05-17 |        4.06 |                 18 |             980 |         870 |          120000 |            72 |          368 |      753 |            7 |             0 |
| mr_trash |        6 | May   | 2014 | 2014-05-20 |        2.71 |                 13 |            1430 |        2140 |           90000 |            46 |          672 |     1144 |            5 |             0 |

Answering questions:

``` r
prof_weight = 
  trash_df %>% 
  filter(type == "professor_trash") %>% 
  pull(weight_tons) %>% 
  sum()

gwyn_cigarette = 
  trash_df %>% 
  filter(type == "gwynnda_trash",
         year == 2022,
         month == "June") %>% 
  pull(cigarette_butts) %>% 
  sum()
```

The total weight of trash collected by Professor Trash Wheel is 246.74
tons.

The total number of cigarette butts collected by Gwynnda in June of 2022
is 18120.

## Problem 3

### Import and clean the data

``` r
bakers_df = 
  read_csv("HW2_data/gbb_datasets/bakers.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    baker_first_name = sub(" .*", "", baker_name)
    )

bakes_df = 
  read_csv("HW2_data/gbb_datasets/bakes.csv", 
           na = c("NA", "N/A", "UNKNOWN", "Unknown", "")) %>% 
  janitor::clean_names() %>% 
  mutate(
    baker = ifelse(baker == "\"Jo\"", "Jo", baker)
    ) %>% 
  rename(baker_first_name = baker) %>% 
  arrange(baker_first_name)


results_df = 
  read_csv("HW2_data/gbb_datasets/results.csv",
           skip = 2) %>% 
  janitor::clean_names() %>% 
  rename(baker_first_name = baker) %>% 
  arrange(baker_first_name) %>% 
  drop_na(result)
```

Data cleaning steps: import dataset -\> clean variable names -\> wrangle
each of datasets

`bakers_df`:

- Pick up the baker’s first name to match the other datasets

`bakes_df`:

- Unknown values are uniformly recorded as NA

- For the baker named `"Jo"`, convert the name into `Jo` to match the
  other datasets

- Rename variable `baker` to `baker_first_name`, to match the first
  dataset `bakers_df`

- Sort baker’s name alphabetically

`results_df`:

- Skip some lines which not contain observations

- Rename variable `baker` to `baker_first_name`, to match the first
  dataset `bakers_df`

- Sort baker’s name alphabetically

- By looking at the dataset, we can see that the eliminated bakers have
  NA in `result` after they are OUT, which means they will no longer
  participate in the competition. Therefore, NA in `result` should be
  dropped.

### Organize bakers datasets

``` r
baker_bake_df = 
  left_join(
    bakers_df, 
    bakes_df, 
    join_by(baker_first_name, series)
  )

baker_organized_df = 
  full_join(
    baker_bake_df,
    results_df,
    join_by(baker_first_name, series, episode)
  ) %>% 
  group_by(baker_first_name, series) %>% 
  fill(baker_name:hometown) %>% 
  ungroup() %>% 
  arrange(baker_first_name) %>% 
  drop_na(episode)
```

1.  Join `bakers.csv` and `bakes.csv` first, by baker’s first name +
    series. The result is **the bakers+bakes dataset**.

2.  Join **the bakers+bakes dataset** with `results.csv`, by baker’s
    first name + series + episode.

3.  By looking at the dataset, we can see some bakers only have episode
    information in `results.csv`, so the rows cannot match with those in
    **the bakers+bakes dataset** (where episode information is NA). To
    fix this, I fill those NAs with `group_by()` and `fill()`, then
    remove the grouping after the operation.

4.  Next, sort baker’s first name alphabetically, and drop the rows that
    have NA in `episode`. (Explanation: These rows are used as reference
    to fill in *step 3*, and only have the information from **the
    bakers+bakes dataset** but not from `results.csv`. Therefore, these
    rows can be dropped after **the bakers+bakes dataset** information
    is filled.)

### Check across datasets

``` r
anti_join(
  bakers_df,
  baker_organized_df,
    join_by(baker_first_name, series)
  )
```

    ## # A tibble: 0 × 6
    ## # ℹ 6 variables: baker_name <chr>, series <dbl>, baker_age <dbl>,
    ## #   baker_occupation <chr>, hometown <chr>, baker_first_name <chr>

``` r
anti_join(
  bakes_df,
  baker_organized_df,
    join_by(baker_first_name, series, episode)
  )
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, baker_first_name <chr>,
    ## #   signature_bake <chr>, show_stopper <chr>

``` r
anti_join(
  results_df,
  baker_organized_df,
    join_by(baker_first_name, series, episode)
  )
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, baker_first_name <chr>,
    ## #   technical <dbl>, result <chr>

Check for completeness and correctness across datasets. View the three
datasets respectively.

### Resulting dataset

``` r
baker_final_df = 
baker_organized_df %>% 
  select(baker_first_name, baker_name, baker_age, baker_occupation, hometown, series, episode, technical, result, signature_bake, show_stopper) %>% 
  rename(baker_full_name = baker_name)

baker_final_df %>% 
write_csv("HW2_data/gbb_datasets/baker_final_df.csv")
```

1.  Organize the dataset in a meaningful order and name variables
    properly.

- Variables: baker’s first name and full name -\> baker’s basic
  information (age, occupation, hometown) -\> series and episode
  involved -\> competition technical and result -\> signature bake and
  show stopper

- Observations: Alphabetical order of baker’s first name

2.  Export the result as a CSV

3.  Brief discussion

``` r
baker_final_df
```

    ## # A tibble: 718 × 11
    ##    baker_first_name baker_full_name baker_age baker_occupation  hometown  series
    ##    <chr>            <chr>               <dbl> <chr>             <chr>      <dbl>
    ##  1 Ali              Ali Imdad              25 Charity worker    Saltley,…      4
    ##  2 Ali              Ali Imdad              25 Charity worker    Saltley,…      4
    ##  3 Ali              Ali Imdad              25 Charity worker    Saltley,…      4
    ##  4 Ali              Ali Imdad              25 Charity worker    Saltley,…      4
    ##  5 Alice            Alice Fevronia         28 Geography teacher Essex         10
    ##  6 Alice            Alice Fevronia         28 Geography teacher Essex         10
    ##  7 Alice            Alice Fevronia         28 Geography teacher Essex         10
    ##  8 Alice            Alice Fevronia         28 Geography teacher Essex         10
    ##  9 Alice            Alice Fevronia         28 Geography teacher Essex         10
    ## 10 Alice            Alice Fevronia         28 Geography teacher Essex         10
    ## # ℹ 708 more rows
    ## # ℹ 5 more variables: episode <dbl>, technical <dbl>, result <chr>,
    ## #   signature_bake <chr>, show_stopper <chr>

- There are 718 rows and 11 columns in the final dataset. The dataset
  shows information of the bakers, their bakes and their performance.

### Winners

``` r
baker_final_df %>% 
  select(series, result, baker_first_name) %>% 
  filter(series >= 5, series <= 10,
         result %in% c("WINNER", "STAR BAKER")) %>%
  mutate(
    result = case_match(
      result,
      "WINNER" ~ "Winner",
      "STAR BAKER" ~ "Star Baker"
    )
  ) %>% 
  arrange(series, result) %>% 
  distinct() %>% 
  pivot_wider(
    names_from = result,
    values_from = baker_first_name,
    values_fn = list
  ) %>% 
  rename(Series = series) %>% 
  knitr::kable()
```

| Series | Star Baker                                   | Winner  |
|-------:|:---------------------------------------------|:--------|
|      5 | Chetna , Kate , Luis , Nancy , Richard       | Nancy   |
|      6 | Ian , Marie , Mat , Nadiya, Tamal            | Nadiya  |
|      7 | Andrew , Benjamina, Candice , Jane , Tom     | Candice |
|      8 | Julia , Kate , Liam , Sophie, Stacey, Steven | Sophie  |
|      9 | Briony , Dan , Kim-Joy, Manon , Rahul , Ruby | Rahul   |
|     10 | Alice , Henry , Michael , Michelle, Steph    | David   |

**The star baker and winner of each episode in Seasons 5 through 10**

Steps:

1.  Select columns and filter rows in interest

2.  Sort by series and baker’s first name

3.  Obtain all the unique lines

4.  Organize variable names

5.  Generate a reader-friendly table

### Comments on winners

``` r
baker_final_df %>% 
  select(series, result, baker_first_name) %>% 
  filter(series >= 5, series <= 10,
         result %in% c("WINNER", "STAR BAKER")) %>%
  arrange(baker_first_name) %>% 
  distinct() %>% 
  knitr::kable()
```

| series | result     | baker_first_name |
|-------:|:-----------|:-----------------|
|     10 | STAR BAKER | Alice            |
|      7 | STAR BAKER | Andrew           |
|      7 | STAR BAKER | Benjamina        |
|      9 | STAR BAKER | Briony           |
|      7 | STAR BAKER | Candice          |
|      7 | WINNER     | Candice          |
|      5 | STAR BAKER | Chetna           |
|      9 | STAR BAKER | Dan              |
|     10 | WINNER     | David            |
|     10 | STAR BAKER | Henry            |
|      6 | STAR BAKER | Ian              |
|      7 | STAR BAKER | Jane             |
|      8 | STAR BAKER | Julia            |
|      5 | STAR BAKER | Kate             |
|      8 | STAR BAKER | Kate             |
|      9 | STAR BAKER | Kim-Joy          |
|      8 | STAR BAKER | Liam             |
|      5 | STAR BAKER | Luis             |
|      9 | STAR BAKER | Manon            |
|      6 | STAR BAKER | Marie            |
|      6 | STAR BAKER | Mat              |
|     10 | STAR BAKER | Michael          |
|     10 | STAR BAKER | Michelle         |
|      6 | STAR BAKER | Nadiya           |
|      6 | WINNER     | Nadiya           |
|      5 | STAR BAKER | Nancy            |
|      5 | WINNER     | Nancy            |
|      9 | STAR BAKER | Rahul            |
|      9 | WINNER     | Rahul            |
|      5 | STAR BAKER | Richard          |
|      9 | STAR BAKER | Ruby             |
|      8 | STAR BAKER | Sophie           |
|      8 | WINNER     | Sophie           |
|      8 | STAR BAKER | Stacey           |
|     10 | STAR BAKER | Steph            |
|      8 | STAR BAKER | Steven           |
|      6 | STAR BAKER | Tamal            |
|      7 | STAR BAKER | Tom              |

To get a deeper understanding of the table, I arranged the table by
baker’s name to see how each person performed.

- Most of them got STAR BAKER once, and a few of STAR BAKER also got
  WINNER.

- What surprise me is that David, the WINNER of season 10, only got
  WINNER but not STAR BAKER. Furthermore, Kate got STAR BAKER twice but
  didn’t get WINNER in both seasons.

- Based on these situations, I think there are many factors that
  determine the overall winner, and we can’t simply judge from the
  results.

### Viewers

``` r
viewers_df = 
  read_csv("HW2_data/gbb_datasets/viewers.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series",
    values_to = "viewership",
    names_prefix = "series_"
  ) %>% 
  mutate(
    series = as.numeric(series)
  ) %>% 
  relocate(series) %>% 
  arrange(series)

head(viewers_df, 10)
```

    ## # A tibble: 10 × 3
    ##    series episode viewership
    ##     <dbl>   <dbl>      <dbl>
    ##  1      1       1       2.24
    ##  2      1       2       3   
    ##  3      1       3       3   
    ##  4      1       4       2.6 
    ##  5      1       5       3.03
    ##  6      1       6       2.75
    ##  7      1       7      NA   
    ##  8      1       8      NA   
    ##  9      1       9      NA   
    ## 10      1      10      NA

``` r
viewers_df %>% 
  filter(series == 1) %>% 
  pull(viewership) %>% 
  mean(, na.rm = TRUE)
```

    ## [1] 2.77

``` r
viewers_df %>% 
  filter(series == 5) %>% 
  pull(viewership) %>% 
  mean(, na.rm = TRUE)
```

    ## [1] 10.0393

The average viewership in Season 1: 2.77

The average viewership in Season 5: 10.0393
